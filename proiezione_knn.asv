clear; clc; close all;

% Carica dati già calcolati
load('volti_dataset.mat');
load('mean_face.mat'); % se lo hai salvato separatamente, altrimenti ricalcali qui
[m, n] = size(A);

% Ricalcola mean_face e SVD se non già salvati
mean_face = mean(A, 2);
A_centered = A - mean_face;
[U, S, ~] = svd(A_centered, 'econ');

% Parametri
k = 50; % numero di eigenfaces da usare
U_k = U(:, 1:k); % prime k eigenfaces

% Proiezione nel sottospazio ridotto
projections = U_k' * A_centered; % matrice k x n

% Divisione training/test: ad esempio 7 immagini per soggetto per training
num_subjects = max(labels);
imgs_per_subject = n / num_subjects;

train_idx = [];
test_idx = [];

for s = 1:num_subjects
    idx = find(labels == s);
    idx = idx(randperm(length(idx))); % shuffle immagini del soggetto

    train_idx = [train_idx, idx(1:7)];
    test_idx = [test_idx, idx(8:end)];
end

% Costruiamo training e test set
X_train = projections(:, train_idx)';
y_train = labels(train_idx)';

X_test = projections(:, test_idx)';
y_test = labels(test_idx)';

% -------------------------
% Rete Neurale
% -------------------------

input_size = size(X_train, 2);   % dimensione input = k
hidden_size = 100;               % neuroni strato nascosto
num_classes = max(y_train);      % numero classi
epochs = 100;
learning_rate = 0.01;

% Inizializza pesi
W1 = randn(hidden_size, input_size) * sqrt(2 / input_size);  % He initialization
b1 = zeros(hidden_size, 1);
W2 = randn(num_classes, hidden_size) * sqrt(2 / hidden_size);
b2 = zeros(num_classes, 1);

% One-hot encoding y_train
Y_train_onehot = zeros(num_classes, length(y_train));
for i = 1:length(y_train)
    Y_train_onehot(y_train(i), i) = 1;
end

% Transponi input
X_train_T = X_train';  % input_size x N

for epoch = 1:epochs
    Z1 = W1 * X_train_T + b1;
    A1 = max(0, Z1);
    Z2 = W2 * A1 + b2;
    expZ = exp(Z2 - max(Z2, [], 1));
    A2 = expZ ./ sum(expZ, 1);

    epsilon = 1e-10;
    loss = -sum(log(sum(A2 .* Y_train_onehot, 1) + epsilon)) / length(y_train);
    if mod(epoch,10) == 0
        fprintf('Epoca %d - Loss: %.4f\n', epoch, loss);
    end

    % Backpropagation
    dZ2 = A2 - Y_train_onehot;
    dW2 = dZ2 * A1' / length(y_train);
    db2 = sum(dZ2, 2) / length(y_train);

    dA1 = W2' * dZ2;
    dZ1 = dA1 .* (Z1 > 0);
    dW1 = dZ1 * X_train_T' / length(y_train);
    db1 = sum(dZ1, 2) / length(y_train);

    % Aggiorna pesi
    W1 = W1 - learning_rate * dW1;
    b1 = b1 - learning_rate * db1;
    W2 = W2 - learning_rate * dW2;
    b2 = b2 - learning_rate * db2;
end


% -------------------------
% Predizione su Test Set
% -------------------------
X_test_T = X_test';

Z1_test = W1 * X_test_T + b1;
A1_test = max(0, Z1_test);
Z2_test = W2 * A1_test + b2;

expZ = exp(Z2_test - max(Z2_test, [], 1));
A2_test = expZ ./ sum(expZ, 1);

[~, y_pred] = max(A2_test, [], 1);
y_pred = y_pred';  % vettore colonna

% Accuratezza
accuracy = sum(y_pred == y_test) / length(y_test);
fprintf('Accuratezza Rete Neurale Manuale: %.2f%%\n', accuracy * 100);